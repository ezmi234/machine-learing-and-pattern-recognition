# PCA
Pca is an unsupervised linear dimensionality reduction technique that identifies the directions (principal components) in which the data varies the most. It is commonly used for data compression, noise reduction, and visualization.

Given a zero-mean dataset $X=\{x_1,\dots,x_k\}$, $x_i \in \mathbb{R}^n$, PCA finds a subspace of dimension $m << n$ that preserves most of the data variance.

A subspace is represented by $P \in \mathbb{R}^{n \times m}$ with orthonormal columns ($P^TP=I$).
A point $x$ projects as $y = P^T x$; the reconstruction is $\hat{x} = P y = P P^T x$.

- The optimal $P$ minimizes the average reconstruction error:
  $$
  P^* = \arg\min_{P} \sum_{i=1}^k ||x_i - \hat{x}||^2 = \arg\min_{P} \sum_{i=1}^k ||x_i - P P^T x_i||^2
  $$

This is equivalent to maximizing $Tr(P^T C P)$ where $C$ is the data covariance matrix:
$$
C = \frac{1}{k} \sum_{i=1}^k x_i x_i^T
$$

- Eigen-decomposition $C=U \Sigma U^T $, $U = [u_1,\dots,u_n]$, $\Sigma = diag(\sigma_1,\dots,\sigma_n)$ with $\sigma_1  \geq \dots \geq \sigma_n$.
- Select the top $m$ eigenvectors: $P^* = [u_1,\dots,u_m]$.

Thus pca preserves the directions of highest variance.

If data is not centered:
- subtract the mean $\bar{x}$ before applying PCA.
- then:
    $$
    C = \frac{1}{k} \sum_{i=1}^k (x_i - \bar{x})(x_i - \bar{x})^T
    $$
PCA decorrelated data: projection over $U^T$ yields uncorrelated components.

- For classification, PCA can be employed to simplify classification by reducing the number of input dimensions, and thus the number of parameters that need to be estimated, for a given classifier, reducing the risk of overfitting.
The optimal number of components $m$ can be chosen by cross-validation.

# LDA 